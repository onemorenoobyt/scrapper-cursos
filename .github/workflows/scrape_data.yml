# Contenido de .github/workflows/scrape_data.yml

name: Scrape Latest Course Data

on:
  schedule:
    # Ejecuta el scraper todos los d칤as a las 6:00 AM UTC.
    - cron: '0 5 * * 1'
  
  workflow_dispatch:
    # Permite ejecutarlo manualmente desde la pesta침a "Actions" de GitHub.

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    # Otorga permisos de escritura al bot de la acci칩n para que pueda hacer 'git push'.
    permissions:
      contents: write

    steps:
      # Paso 1: Descargar tu c칩digo desde el repositorio a la m치quina virtual.
      - name: Checkout repository
        uses: actions/checkout@v3

      # --- NUEVO PASO A칌ADIDO ---
      # Paso 2: Instalar el navegador Google Chrome en la m치quina virtual.
      # Selenium lo necesita para poder ejecutarse.
      - name: Set up Chrome
        uses: browser-actions/setup-chrome@v1
      # -------------------------

      # Paso 3: Instalar Python en la m치quina virtual.
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      # Paso 4: Instalar todas las librer칤as de Python necesarias (incluyendo Selenium).
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Paso 5: Ejecutar nuestro script principal que lanza todos los scrapers.
      - name: Run the scraper
        run: python main.py

      # Paso 6: Guardar el fichero CSV actualizado en el repositorio si ha habido cambios.
      - name: Commit and push if there are changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add cursos_actualizados.csv
          git diff --quiet && git diff --staged --quiet || (git commit -m "游늵 Actualizaci칩n autom치tica de datos de cursos" && git push)